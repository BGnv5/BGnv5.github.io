{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/03/20/hello-world/"},{"title":"test","text":"模板 title: 使用Hexo搭建个人博客 layout: post date: 2014-03-03 19:07:43 comments: true categories: Blog tags: [Hexo] keywords: Hexo, Blog description: 生命在于折腾，又把博客折腾到Hexo了。给Hexo点赞。","link":"/2020/03/24/test/"},{"title":"LeetCode 148.Sort List","text":"sort-list题目描述：在O(n log n)的时间内使用常数级空间复杂度对链表进行排序。Sort a linked list in O(n log n) time using constant space complexity. 思路分析：1. 归并排序（递归法）： 如果链表为空，或者只有一个结点则直接返回这个链表 不为空，利用快慢指针把链表为成左右两部分，当快指针所指向结点不为空并且存在下一个元素时，快指针走两步，慢指针走一步。 慢指针所指向的下一个结点即为右半部分的开始，并把slow指向的下一个结点置为空。 依次递归直到左右两半部分只含有一个结点为止。 最后在进行归并。 时间复杂度：O(n log n)空间复杂度：O(n) 2. 归并排序（迭代法）： 定义一个头结点，让这个头结点的next指向head 统计链表中一共有几个结点，并遍历整个链表 每次截取长度为size的链表进行归并，每轮过后size长度*2，直到size长度等于链表的长度，设size初始值为1。 最后返回头结点指向的下一个结点 时间复杂度：O(n log n)空间复杂度：O(1) 算法实现：1.归并排序（递归法）： 123456789101112131415161718192021222324252627282930313233public class Solution { public ListNode sortList(ListNode head) { if(head == null || head.next == null) return head; ListNode slow = head; ListNode fast = head.next; while(fast != null &amp;&amp; fast.next != null){ slow = slow.next; fast = fast.next.next; } ListNode mid = slow.next; slow.next = null; ListNode left = sortList(head); ListNode right = sortList(mid); ListNode temp = new ListNode(0); ListNode p = temp; while(left != null &amp;&amp; right != null){ if(left.val &lt; right.val){ p.next = left; left = left.next; }else{ p.next = right; right = right.next; } p = p.next; } if(left != null) { p.next = left; }else{ p.next = right; } return temp.next; }} 2. 归并排序（迭代法） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Solution { public ListNode sortList(ListNode head) { //求链表长度 ListNode p = head; int len = 0; while(p != null){ len ++; p = p.next; } ListNode pHead = new ListNode(0); pHead.next = head;//主要是考虑到只有一个结点的情况 for(int size = 1; size &lt; len; size *= 2){ ListNode cur = pHead.next;//一轮结束cur要回到链表首部 ListNode next = pHead;//一轮结束next回到初始位置 while(cur != null){ ListNode left = cur; ListNode right = cut(left,size); cur = cut(right,size); next.next = merge(left,right); while(next.next != null){ next = next.next; } } } return pHead.next; } //根据步长分割链表 private ListNode cut(ListNode head, int size){ if(head == null) return head; for (int i = 1; head.next != null &amp;&amp; i &lt; size; i++) { head = head.next; } ListNode nextHead = head.next; head.next = null; return nextHead; } //归并链表 private ListNode merge(ListNode left, ListNode right){ ListNode head = new ListNode(0); ListNode p = head; while(left != null &amp;&amp; right != null){ if(left.val &lt; right.val){ p.next = left; left = left.next; }else{ p.next = right; right = right.next; } p = p.next; } if(left != null) p.next = left; else p.next = right; return head.next; }}","link":"/2020/03/25/LeetCode-148-Sort-List/"},{"title":"LeetCode 147.insert-sort-list","text":"insert-sort-list题目描述：使用插入排序对链表进行排序。Sort a linked list using insertion sort. 思路分析： 如果链表为空,返回null 链表非空，定义一个头结点，遍历整个链表 定义一个变量pre用来指示应该插入的位置，如果这个变量所在位置的值小于待插入元素的值，并且这个变量存在下一个结点，则变量的位置后移一位，最终所在位置即为待插入的位置。 用一个变量cur来指示待插入数所在位置，并修改待插入数所指向下一个结点的位置,让它等于pre.next 修改带插入位置的下一个结点,让它等于cur 算法实现：123456789101112131415161718public class Solution { public ListNode insertionSortList(ListNode head) { if(head == null) return null; ListNode pHead = new ListNode(0); ListNode cur = head; while(cur != null){ ListNode pre = pHead; ListNode next = cur.next; while(pre.next != null &amp;&amp; pre.next.val &lt; cur.val){ pre = pre.next; } cur.next = pre.next; pre.next = cur; cur = next; } return pHead.next; }}","link":"/2020/03/25/LeetCode-147-insert-sort-list/"},{"title":"史上最详细的hive安装教程，避免踩坑系列","text":"网上安装教程很多，照着安装还会出现各种各样的错误，这篇文章手把手教你安装hive，对安装时常见的几种错误也进行了汇总，让你能够轻松避免踩坑~ Hive 安装部署derby版 Hive前提：安装配置好 Jdk 和 hadoop,并启动 hdfs。 下载安装包,并上传到/opt/software下 地址：[https://mirrors.tuna.tsinghua.edu.cn/apache/hive/] 解压hive压缩包到/opt/module下： 1tar -zxvf apache-hive-1.2.1-bin.tar.gz -C ../module 将文件重命名为hive-1.2.1文件： 1mv apache-hive-1.2.1-bin/ hive-1.2.1 进入bin目录直接启动 12[root@hadoop100 software]# cd ../module/hive-1.2.1/[root@hadoop100 hive-1.2.1]# bin/hive 测试一下，查看数据库，出现如下所示即安装成功！ 1234hive&gt; show databases;OKdefaultTime taken: 1.63 seconds, Fetched: 1 row(s) 注意： 如果启动hive时如果报: 1Call From hadoop /192.168.1.100 to hadoop :9000 failed on connection 解决方案： 进入hadoop/bin目录下，格式化namenode 12[root@hadoop100 module]# cd hadoop-2.7.1/bin[root@hadoop100 bin]# hadoop namenode -format 然后进入hadoop/sbin 执行 sh stop-all.sh 12[root@hadoop100 bin]# cd ../sbin[root@hadoop100 sbin]# sh stop-all.sh 最后重新启动 sh start-all.sh即可 1[root@hadoop100 sbin]# sh start-all.sh 缺点：使用内置 derby版，不同路径启动hive，每个hive拥有一套自己的元数据，无法共享。 Mysql版 Hive安装并配置MySQL 查看是否安装MySql1rpm -qa|grep -i mysql 如果已经安装过,卸载安装过MySQL:1rpm -ev --nodeps mysql-libs-5.1.71-1.el6.x86_64 删除其他配置12rm -rf /usr/my.cnfrm -rf /root/.mysql_sercret 下载并安装MySQL官方的 Yum Repository 1wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm 使用 yum 进行安装 1yum -y install mysql57-community-release-el7-10.noarch.rpm 安装MySQL（完成后就会覆盖掉之前的mariadb） 1yum -y install mysql-server mysql 启动MySql服务 1[root@hadoop100 ~]# systemctl start mysqld.service 启动mysql时报错： 1Job for mysqld.service failed because the control process exited with error code. See \"systemctl status mysqld.service\" and \"journalctl -xe\" for details. 解决方法： 修改/var/lib/mysql 文件的权限1[root@hadoop100 ~]# chmod -R 777 /var/lib/mysql 删除/var/lib/mysql 下的所有文件1[root@hadoop100 ~]# rm -rf /var/lib/mysql/* 重新启动mysql服务1[root@hadoop100 ~]# systemctl start mysqld.service 查看MySQL运行状态,出现下图所示即启动成功1[root@hadoop100 ~]# systemctl status mysqld.service 12345678910111213● mysqld.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since 二 2020-03-31 10:29:00 CST; 24s ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Process: 4054 ExecStart=/usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid $MYSQLD_OPTS (code=exited, status=0/SUCCESS) Process: 4005 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS) Main PID: 4057 (mysqld) CGroup: /system.slice/mysqld.service └─4057 /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid3月 31 10:28:51 hadoop100 systemd[1]: Starting MySQL Server...3月 31 10:29:00 hadoop100 systemd[1]: Started MySQL Server. 登入MySQL1[root@hadoop100 ~]# mysql -u root -p 连接MySQL报错： 1ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO) 解决方法： 查看root用户的密码1[root@hadoop100 ~]# grep \"password\" /var/log/mysqld.log 12345678910111213141516171819202122232425262728293031323334353637382020-03-30T17:20:05.867355Z 1 [Note] A temporary password is generated for root@localhost: Qsye(dZPq2020-03-30T17:30:01.493042Z 2 [Note] Access denied for user 'root'@'localhost' (using password: YES)2020-03-30T17:30:43.027818Z 3 [Note] Access denied for user 'root'@'localhost' (using password: YES)2020-03-30T17:30:58.234454Z 4 [Note] Access denied for user 'root'@'localhost' (using password: YES)2020-03-30T17:31:29.410990Z 5 [Note] Access denied for user 'root'@'localhost' (using password: YES)2020-03-30T17:31:43.170470Z 6 [Note] Access denied for user 'root'@'localhost' (using password: YES)2020-03-30T17:33:41.321386Z 8 [Note] Access denied for user 'root'@'localhost' (using password: YES)2020-03-30T17:50:55.265575Z 11 [Note] Access denied for user 'root'@'localhost' (using password: YES2020-03-30T19:27:01.508362Z 0 [Note] Shutting down plugin 'validate_password'2020-03-30T19:27:04.113128Z 0 [Note] Shutting down plugin 'sha256_password'2020-03-30T19:27:04.113136Z 0 [Note] Shutting down plugin 'mysql_native_password'2020-03-31 04:30:50 3851 [Note] Shutting down plugin 'sha256_password'2020-03-31 04:30:50 3851 [Note] Shutting down plugin 'mysql_old_password'2020-03-31 04:30:50 3851 [Note] Shutting down plugin 'mysql_native_password'2020-03-31 04:45:55 6906 [Note] Shutting down plugin 'sha256_password'2020-03-31 04:45:55 6906 [Note] Shutting down plugin 'mysql_old_password'2020-03-31 04:45:55 6906 [Note] Shutting down plugin 'mysql_native_password'2020-03-31 04:55:55 8956 [Note] Shutting down plugin 'sha256_password'2020-03-31 04:55:55 8956 [Note] Shutting down plugin 'mysql_old_password'2020-03-31 04:55:55 8956 [Note] Shutting down plugin 'mysql_native_password'2020-03-31 05:06:10 11074 [Note] Shutting down plugin 'sha256_password'2020-03-31 05:06:10 11074 [Note] Shutting down plugin 'mysql_old_password'2020-03-31 05:06:10 11074 [Note] Shutting down plugin 'mysql_native_password'2020-03-31 05:15:11 13422 [Note] Shutting down plugin 'sha256_password'2020-03-31 05:15:11 13422 [Note] Shutting down plugin 'mysql_old_password'2020-03-31 05:15:11 13422 [Note] Shutting down plugin 'mysql_native_password'2020-03-31 05:25:12 15960 [Note] Shutting down plugin 'sha256_password'2020-03-31 05:25:12 15960 [Note] Shutting down plugin 'mysql_old_password'2020-03-31 05:25:12 15960 [Note] Shutting down plugin 'mysql_native_password'2020-03-31 09:39:35 2385 [Note] Shutting down plugin 'sha256_password'2020-03-31 09:39:35 2385 [Note] Shutting down plugin 'mysql_old_password'2020-03-31 09:39:35 2385 [Note] Shutting down plugin 'mysql_native_password'2020-03-31T02:28:54.650273Z 1 [Note] A temporary password is generated for root@localhost: lts2L)L&lt;h2020-03-31T02:30:15.184424Z 2 [Note] Access denied for user 'root'@'localhost' (using password: NO)2020-03-31T02:30:24.888102Z 3 [Note] Access denied for user 'root'@'localhost' (using password: NO)2020-03-31T02:30:44.372240Z 4 [Note] Access denied for user 'root'@'localhost' (using password: YES)2020-03-31T02:34:40.134792Z 5 [Note] Access denied for user 'root'@'localhost' (using password: YES)2020-03-31T02:34:49.375071Z 6 [Note] Access denied for user 'root'@'localhost' (using password: YES) 根据查找到的密码重新登入即可12345[root@hadoop100 ~]# mysql -uroot -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 12Server version: 5.7.29 修改密码（这里用户名为root，密码为BGnv5_Oooh）1mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'BGnv5_Oooh'; 注意: 密码设置必须要大小写字母数字和特殊符号（,/‘;:等）,不然不能配置成功 使用mysql数据库 1mysql&gt; use mysql; 把在所有数据库的所有表的所有权限赋值给位于所有IP地址的root用户 1mysql&gt; update user set host = '%' where user = 'root'; 配置远程连接 1mysql&gt; grant all privileges on *.* to root@'%'identified by 'BGnv5_Oooh' with grant option; 刷新MySQL的系统权限相关表 12mysql&gt; flush privileges;mysql&gt; exit; 安装并配置hive 下载hive压缩包并解压到/opt/module下： 1tar -zxvf apache-hive-1.2.1-bin.tar.gz -C ../module 配置HIVE环境变量 1234567[root@hadoop100 ~]vim /etc/profile##Hiveexport HIVE_HOME=/opt/module/hive-1.2.1export PATH=$PATH:$HIVE_HOME/bin[root@hadoop100 ~]# source /etc/profile 进入hive/conf 目录下1[root@hadoop100 ~]# cd /opt/module/hive-1.2.1/conf/ 配置hive-env.sh文件 12345678[root@hadoop100 conf]# cp hive-env.sh.template hive-env.sh[root@hadoop100 conf]# vim hive-env.sh# Set HADOOP_HOME to point to a specific hadoop install directoryexport HADOOP_HOME=/opt/module/hadoop-2.7.1# Hive Configuration Directory can be controlled by:export HIVE_CONF_DIR=/opt/module/hive-1.2.1/conf 配置 hive-site.xml文件(默认不存在) 12345678910111213141516171819202122[root@hadoop100 conf]# vim hive-site.xml&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;BGnv5_Oooh&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 将驱动包mysql-connector-java-5.1.38.jar 放在hive/lib目录下 1[root@hadoop100 conf]# cd /opt/module/hive-1.2.1/lib 对MySQL 进行初始化 1[root@hadoop100 conf]# schematool -dbType mysql -initSchema 启动hadoop集群 12[root@hadoop100 conf]# cd /opt/module/hadoop-2.7.1/[root@hadoop100 hadoop-2.7.1]# sbin/start-all.sh 启动Hve 1[root@hadoop100 conf]# hive 测试hive 在hive/conf目录下创建一个BGnv5数据库，并查看数据库123hive&gt; create database BGnv5;OKTime taken: 1.317 seconds 12345hive&gt; show databases;OKbgnv5defaultTime taken: 0.664 seconds, Fetched: 2 row(s) 切换到主目录,在主目录处启动hive，并查看数据库 1[root@hadoop100 conf]# cd 1[root@hadoop100 ~]# hive 12345hive&gt; show databases;OKbgnv5defaultTime taken: 1.565 seconds, Fetched: 2 row(s) 可以观察到不论是在哪里启动hive，数据都可以共享，MySQL版hive安装成功。","link":"/2020/03/31/%E5%8F%B2%E4%B8%8A%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84hive%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B%EF%BC%8C%E9%81%BF%E5%85%8D%E8%B8%A9%E5%9D%91%E7%B3%BB%E5%88%97/"},{"title":"Hive基本介绍","text":"什么是Hive？我们为什么要使用Hive？Hive的架构以及工作流程是怎样的？什么是内部表什么是外部表？他们有什么区别？什么是分区表和分桶表？他们有什么区别？ Hive 简介什么是Hive?&emsp;&emsp;hive是由 Facebook 实现并开源的一个基于Hadoop的数据仓库工具，它的底层数据是存储在HDFS上的，它提供了类似SQL的 Hive SQL语言，能够将结构化的数据映射为一张数据库表，其底层原理是将HQL 语言转化为MapReduce 任务执行，从而完成对Hadoop集群中存储的海量数据进行查询和分析。 为什么使用Hive?&emsp;&emsp;hive提供了类SQL的语法，避免了写实现复杂逻辑的MapReduce程序，从而降低了开发人员的学习成本，提高开发人员快速开发的能力，并且hive还具有更好的扩展性，可以自由扩展集群规模而无需重启服务，还支持用户自定义函数。 Hive的优缺点优点： 简单易上手：提供了类SQL查询语言HQL 可扩展：Hive可以自由的扩展集群的规模，一般情况下不需要重启服务就可以进行扩展 提供了同一的元数据管理 延展性：Hive支持自定义函数，用户可以根据自己的需求来实现自己的函数 容错性：良好的容错性，结点出现故障，SQL语句仍可以完成执行 缺点： Hive延时严重：启动MapReduce Job时有延迟 Hive不支持记录级别的增删改操作，但是用户可以通过查询生成新表或者将查询结果导入到文件中 Hive不支持事务：主要用来做OLAP(联机分析处理)，而不适用OLTP（联机事务处理） Hive 与RDBMS 的对比 Hive使用HDFS存储数据，而关系型数据库则是存储在服务器本地的文件系统中； Hive使用MapReduce作为计算模型，而关系型数据库则是使用自己设计的计算模型 Hive实时性差它是为海量数据做数据挖掘设计的；而关系型数据库实时性好，它是为实时查询的业务进行设计的 Hive扩展性强，可以很容易的扩展自己的存储能力和计算能力，而关系型数据库在这方面相对较差 Hive不支持对某个具体行的操作，对数据的操作只支持覆盖原数据和追加数据；而关系型数据库支持对行级的更新删除 Hive是”读时模式”，即在加载数据的时候不会对数据进行检查/更改，在查询操作时才会检查数据的格式；而关系型数据库是”写时模式”，在数据加载的时候就会对数据模式进行检查校验的操作。 Hive延迟较高，因为Hive不支持索引，所以查询的时候会扫描整个表，而且Hive在启动MapReduce Job的时候也会造成较大的延迟；而关系型数据库支持复杂索引，所以延迟相对较小 Hive 架构 Hive架构设计Hive架构设计主要包括三个部分： Hive Clients：Hive客户端，它为不同类型的应用程序提供不同的驱动，使得Hive可以通过类似Java、Python等语言连接，同时也提供了JDBC和ODBC驱动。 Hive Services：Hive服务端，客户端必须通过服务端与Hive交互，主要包括：&emsp;&emsp;用户接口组件（CLI，HiveServer，HWI），它们分别以命令行、与web的形式连接Hive。&emsp;&emsp;Driver组件，该组件包含编译器、优化器和执行引擎，它的作用是将hiveSQL语句进行解析、编译优化、生成执行计划，然后调用底层MR计算框架。&emsp;&emsp;Metastore组件，元数据服务组件。Hive数据分为两个部分，一部分真实数据保存在HDFS中，另一部分是真实数据的元数据，一般保存在MySQL中，元数据保存了真实数据的很多信息，是对真实数据的描述。 Hive Storage and Computing:包括元数据存储数据库和Hadoop集群。Hive元数据存储在RDBMS中，Hive数据存储在HDFS中，查询由MR完成。 Hive各组件说明 thrift server：跨语言服务提供了一种能力，让用户可以使用多种不同的语言来操纵hive 用户接口: shell/CLI, jdbc/odbc, webui Command Line Interface CLI，Shell 终端命令行（Command Line Interface），采用交互形式使用 Hive 命令行与 Hive 进行交互，最常用（学习，调试，生产） JDBC/ODBC，是Hive基于JDBC操作提供的客户端，用户（开发员，运维人员）通过这连接至 Hive server 服务 Web UI，通过浏览器访问 Hive 驱动器Driver：它是Hive的核心,主要完成HQL查询语句从词法分析，语法分析，编译，优化，以及逻辑执行计划的生成。生成的逻辑执行计划存储在HDFS中，并随后由MapReduce调用执行。 &emsp;&emsp;Driver驱动引擎由四部分组成： &emsp;&emsp;&emsp;&emsp;解释器：解释器的作用是将HiveSQL语句转换为抽象语法树（AST）&emsp;&emsp;&emsp;&emsp;编译器Compiler：编译器是将语法树编译为逻辑执行计划&emsp;&emsp;&emsp;&emsp;优化器Optimizer：优化器是对逻辑执行计划进行优化&emsp;&emsp;&emsp;&emsp;执行器Executor：执行器是调用底层的运行框架执行逻辑执行计划 元数据存储系统RDBMS MySQL：存储在 Hive 中的数据的描述信息 Hive 中的元数据通常包括：表的名字，表的列和分区及其属性，表的属性（内部表和外部表），表的数据所在目录等 Metastore 默认存在自带的Derby数据库中。缺点就是不适合多用户操作，数据存储目录不固定。数据库跟着Hive走，极度不方便管理。因此建议存储在关系型数据库中 Hive 具体工作流程 1）用户通过用户接口提交查询任务给Driver。 2）Driver获得该用户的计划，调用编译器检查查询语法并分析查询计划和要求。 3）编译器Compiler根据用户任务到MetaStore中请求需要的Hive的元数据信息。 4）Metastore发送元数据信息给编译器 5）编译器对任务进行编译，先将HiveQL转换为抽象语法树，然后将抽象语法树转换成查询块，将查询块转化为逻辑的查询计划，重写逻辑查询计划，将逻辑计划转化为物理的计划（MapReduce）,最后选择最佳的策略。并将计划提交给Driver。 6）Driver将计划转交给执行引擎ExecutionEngine去执行 7）执行引擎发送作业给JobTracker，执行引擎获取元数据信息（这是在名称节点），并把它分配作业到TaskTracker（这是在数据节点），在这里查询执行MapReduce工作。与此同时，在执行时，执行引擎可以通过Metastore执行元数据操作（虚线部分）。 8）执行引擎接收来自数据节点的执行结果。 9）执行引擎发送这些结果给驱动程序。 10）驱动程序将结果发送给Hive接口（返回给用户） Hive 中的数据模型： database：在 HDFS中表现为${hive.metastore.warehouse.dir}目录下一个文件夹 table：在 HDFS 中表现所属 database 目录下一个文件夹 external table：与 table 类似，不过其数据存放位置可以指定任意 HDFS 目录路径 partition：在 HDFS 中表现为 table 目录下的子目录 bucket：在 HDFS 中表现为同一个表目录或者分区目录下根据某个字段的值进行 hash 散列之后的多个文件 view：与传统数据库类似，只读，基于基本表创建 Hive 中的内部表、外部表什么是内部表和外部表 hive有两种类型的表：内部表、外部表 如果在创建表的时候没有指明是内部表还是外部表，则默认是内部表 内部表和外部表的区别：内部表： 内部表的数据会放在hdfs中的特定位置中，通常是/user/hive/warehouse 当使用drop table table_name;删除内部表时，数据文件也会一并删除 外部表： 外部表的数据的可以存放在 hdfs任意路径下 当使用drop table table_name;删除外部表时，只是删除了表的元数据，它的数据并没有被删除 内部表和外部表的使用场景内部表： 适用于临时创建的中间表。 如果数据的所有处理都在 Hive 中进行，那么倾向于选择内部表。 外部表： 适用于数据多部门共享。 如果 Hive 和其他工具要针对相同的数据集进行处理，外部表更合适。使用外部表访问存储在 HDFS 上的初始数据，然后通过 Hive 转换数据并存到内部表中。 Hive中的分区表和分桶表为什么要进行分区和分桶分区： 分区的最大好处就是可以更快的执行查询。 在分区的帮助下，将使用分区列的名称创建一个子目录，并且当使用where子句执行查询时，将只扫描特定的子目录，而不是扫描整个表。能够更快地执行查询。 分桶： 分桶表是一种更细粒度的数据分配方式，一个表既可以分区也可以分桶。 对表进行分桶操作有利于实现数据的抽样，方便进行数据测试 分区表和分桶表的区别 分区: 是以字段的形式在表结构中存在，通过describe table命令可以查看到字段存在， 但是该字段不存放实际的数据内容，仅仅是分区的表示（伪列）。 分桶:对于表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。实际使用比较少。","link":"/2020/04/02/Hive%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/"},{"title":"hive基本操作(一)","text":"包括Hive的DDL操作、DML操作以及Hive的join操作 Hive 基本操作（一）DDL操作创建表 创建内部表1hive&gt; create table if not exists student1 (sno int,sname string,age int,sex string) row format delimited fields terminated by ' '; 创建外部表1hive&gt; create external table if not exists student (sno int,sname string,age int,sex string) row format delimited fields terminated by ' ' location '/user/external'; 创建与已知表相同结构的表 123##copy_student1表的结构和student1相同但是并不包含student1中的数据hive&gt; create table copy_student1 like student1; 创建分区表 1hive&gt; create table book(id int,name string) partitioned by (country string) row format delimited fields terminated by ' '; 创建分桶表 12345678##桶操作是通过clustered by实现的，bucket中的数据可以通过sort by排序。##分桶表不允许以外部文件方式导入数据，只能从另外一张表导入数据//1.开启分桶机制hive&gt; set hive.enforce.bucketing=true;//2.创建分桶表teacher，以id作为分桶机制，分为2个桶hive&gt; create table teacher(id int,name string,age int) clustered by(id) into 2 buckets row format delimited fields terminated by ' '; 创建Struct类型的表 123##导入数据格式：1,zhou:30hive&gt; create table student_struct(id int,info struct&lt;name:string,age:int&gt;) row format delimited fields terminated by ',' collection items terminated by ':'; 创建Array类型的表 123##导入数据的格式：034,1:2:3:4hive&gt; create table course_array(name string,course_id array&lt;int&gt;) row format delimited fields terminated by ',' collection items terminated by ':'; 创建Map类型的表 123##导入数据的格式：1 job:80,team:60,person:70 hive&gt; create table employee(id string,perf map&lt;string,int&gt;) row format delimited fields terminated by '\\t' collection items terminated by',' map keys terminated by ':'; 修改表 添加列 1hive&gt; alter table student1 add columns(address string,grade string); 修改列 123##将test表中name字段重命名为sname,类型为string，放到age字段后面hive&gt; alter table test change name sname string after age; 修改表名 1hive&gt; alter table students rename to student2; 修改分区名 1hive&gt; alter table book partition(country='us') rename to partition(country='US'); 增加分区 1hive&gt; alter table book add partition(country='jp'); 删除分区 1hive&gt; alter table book drop partition(country='jp'); 修复分区 1hive&gt; msck repair table book; 删除表1hive&gt; drop table test1; 显示命令 查看数据库 1hive&gt; show databases; 查看数据表 1hive&gt; show tables; 查看表的结构 1hive&gt; desc student1; 查看表中都有哪些分区 1hive&gt; show partitions book; 查看表的信息 1hive&gt; describe formatted table_name； DML操作 Load 加载本地数据到普通表中1hive&gt; load data local inpath '/home/bgnv5/data/student1.txt' into table student1; 加载本地数据到分区表中1hive&gt; load data local inpath '/home/bgnv5/data/book.txt' into table book partition(country='cn'); 加载HDFS中的文件到表中1hive&gt; load data inpath'/user/hive/student1.txt' into table copy_student1; insert 将表中的数据插入到一张表中 1hive&gt; insert overwrite table copy_student2 select * from student1; 将表中的数据插入到一个分区中 1hive&gt; insert overwrite table book partition(country='jp') select id,name from book_input where country='jp'; 将表中的数据插入到多张表中 12345hive&gt; from student1 &gt; insert overwrite table copy_student3 &gt; select * &gt; insert overwrite table copy_student4 &gt; select *; 将表中的数据插入到多个分区中 12345hive&gt; from book_input &gt; insert overwrite table book partition(country='英国') &gt; select id,name where country='英国' &gt; insert overwrite table book partition(country='俄罗斯') &gt; select id,name where country='俄罗斯'; 使用动态分区将表中的数据自动插入到分区中 12345678//1.开启动态分区，默认是falsehive&gt; set hive.exec.dynamic.partition=true;//2.开启允许所有分区都是动态的，否则必须要有静态分区才能使用。hive&gt; set hive.exec.dynamic.partition.mode=nonstrict;//3.插入数据hive&gt; insert overwrite table book partition(country) select id,name,country from book_input; 将表中的数据插入到本地目录下 1hive&gt; insert overwrite local directory '/home/bgnv5/data_test' row format delimited fields terminated by ' ' select * from book_input; 将表中的数据插入到hdfs目录下 1hive&gt; insert overwrite directory '/stu' row format delimited fields terminated by ' ' select * from student1; select 查询表的所有字段 1hive&gt; select * from student1; 查询表的某个字段属性 1hive&gt; select sname from student1; 查看某一具体分区 1hive&gt; select * from book where country='cn'; where条件查询 1hive&gt; select * from student1 where sno&gt;201501004 and address=\"北京\"; distinct去重查询 123##查询student1表中age和grade都不相同的学生hive&gt; select distinct age,grade from student1; limit限制查询 123##只显示4条记录hive&gt; select * from student1 limit 4; Struct类型查询 1select id,info.name,info.age from student_struct; Array类型查询 1select name,course_id[0],size(course_id) from course_array; Map类型查询 1select id,pref['job'],size(pref) from employee; order by 排序查询 12345678##全局排序，默认升序##只有一个reduce，即使设置多个reduce个数hive在运行的时候也会设置成1个.##可以通过设置hive.mapred.mode属性优化查询速度，设置此模式必须指定limit否则会报错//hive&gt; select * from student1 order by age;hive&gt; set hive.mapred.mode=strict;hive&gt; select * from student1 order by age limit 5; group by分组查询 123##必须配合聚合函数使用,且可以同时做多个聚合操作，但是聚合操作的对象必须是同一列hive&gt; select count(*) from student1 group by sex; sort by 123456##同一个reduce中的数据是有序的，不同的reduce则不能保证有序//1.设置reduce个数hive&gt; set mapred.reduce.tasks=2;//2.使用sort by 进行排序hive&gt; select * from student1 sort by age; distribute by分区排序 1234567##按照指定的字段把数据划分到不同的reduce中##与sort by 同时使用时要放到前面//1.设置reduce个数hive&gt; set mapred.reduce.tasks=2;//2.按照性别划分到不同的reduce中进行排序，hive&gt; select * from student1 distribute by age; cluster by簇排序 1234567##只能升序##包含了distribute by 和sort by 两个功能##当 distribute by 和 sorts by 字段相同时，可使用 cluster by 方式替代，如：hive&gt; select * from student1 cluster by age;等价于hive&gt; select * from student1 distribute by age sort by age; 分桶表的抽样查询 12345##假设桶数为n,则bucket x out of y on XXX表示为：##从第x个桶开始抽样，抽取n/y个桶的数据##例如，n=6,x=1,y=3抽取两个桶的数据，因此抽取的样本为1和4桶中的数据 hive&gt; select * from teacher tablesample(bucket 1 out of 2 on name); Hive的join操作 内关联 inner join1234567891011121314151617181920hive&gt; select * from a;OK1 zhangsan2 wangwu3 lisiTime taken: 0.23 seconds, Fetched: 3 row(s)hive&gt; select * from b;OK1 202 184 216 23Time taken: 0.129 seconds, Fetched: 4 row(s)hive&gt; select a.id,a.name,b.age from a join b on(a.id=b.id);OK1 zhangsan 202 wangwu 18Time taken: 75.971 seconds, Fetched: 2 row(s) 左外关联12345678##以左表为基础，如果右侧表可以和左侧表关联，则为右侧表值，否则为nullhive&gt; select a.id,a.name,b.age from a left join b on(a.id=b.id);OK1 zhangsan 202 wangwu 183 lisi NULLTime taken: 51.883 seconds, Fetched: 3 row(s) 右外关联123456789##以右表为基础，如果左侧表可以和右侧表关联，则值为左侧表值，否则为NULLhive&gt; select b.id,a.name,b.age from a right join b on(a.id=b.id);OK1 zhangsan 202 wangwu 184 NULL 216 null 23Time taken: 45.56 seconds, Fetched: 4 row(s) 全关联12345678910##两个表进行关联，如果右表没有值补充NULL、如果左边没有值补充NULLhive&gt; select stu.sno,stu.sname,msg.age from stu full join msg on (stu.sno=msg.sno);OK1 zhangsan 202 wangwu 183 lisi NULLNULL NULL 21NULL NULL 23Time taken: 152.029 seconds, Fetched: 4 row(s) left semi join12345##左边表返回key在右边表中存在的值对应的数据hive&gt; select * from a left semi join b on(a.id=b.id);1 zhangsan2 wangwu 笛卡尔积关联 cross join123456789101112131415hive&gt; select a.id,a.name,b.age from a cross join b;OK1 zhangsan 201 zhangsan 181 zhangsan 211 zhangsan 232 wangwu 202 wangwu 182 wangwu 212 wangwu 233 lisi 203 lisi 183 lisi 213 lisi 23Time taken: 43.367 seconds, Fetched: 12 row(s)","link":"/2020/04/07/hive%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"Flume企业面试题","text":"一、你是如何实现Flume数据传输的监控的使用第三方框架Ganglia实时监控Flume。 二、Flume的Source，Sink，Channel的作用？你们Source是什么类型？ 作用 :（1）Source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括netcat、exec、spooling directory、avro、thrift、jms、sequence generator、syslog、http、legacy等（2）Channel组件对采集到的数据进行缓存，可以存放在Memory或File中。（3）Sink组件是用于把数据发送到目的地的组件，目的地包括Hdfs、Logger、avro、thrift、ipc、file、Hbase、solr、自定义。 我公司采用的Source类型为：（1）监控后台日志：exec（2）监控后台产生日志的端口：netcat 三、Flume 的 Channel SelectorsChannel Selectors可以让不同的项目日志通过不同的Channel到不同的Sink中去。 官方文档上提供了两种Chennel Selectors类型：Replicating Channel Selector(default)和Multiplexing Channel Selector 这两种Selector的区别是Replicating会将source过来的events发往所有channel，而Multiplexing 可以选择发往哪些Channel。 四、Flume 参数调优 Source 增加Source个数（使用Tair Dir Source时可增加FileGroups个数）可以增大Source的读取数据的能力。 例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个 Source 以保证 Source 有足够的能力获取到新产生的数据。 batchSize 参数决定 Source 一次批量运输到 Channel 的event条数，适当调大这个参数可以提高 Source 搬运 Event 到 Channel 时的性能。 Channel type 选择 memory 时 Channel 的性能最好，但是如果 Flume 进程意外挂掉可能会丢失数据。type 选择 file 时 Channel 的容错性更好，但是性能上会比 memory channel 差。 使用file Channel时 dataDirs 配置多个不同盘下的目录可以提高性能。 Capacity 参数决定 Channel 可容纳最大的 event 条数。transactionCapacity 参数决定每次 Source 往 channel 里面写的最大event 条数和每次 Sink 从channel 里面读的最大 event 条数。transactionCapacity 需要大于 Source 和Sink的batchSize 参数。 Sink 增加 Sink 的个数可以增加 Sink 消费 event 的能力。Sink 也不是越多越好够用就行，过多的 Sink 会占用系统资源，造成系统资源不必要的浪费。 batchSize 参数决定 Sink 一次批量从 Channel 读取的 event 条数，适当调大这个参数可以提高 Sink 从 Channel 搬出 event 的性能。 五、Flume 的事务机制Flume的事务机制（类似数据库的事务机制）： Flume 使用两个独立的事务分别负责从 Source 到 Channel（put），以及从 Channel 到Sink 的事件传递（take）。 比如 spooling directory source 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到 Channel 且提交成功，那么 Source 就将该文件标记为完成。 同理，事务以类似的方式处理从 Channel 到 Sink 的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到 Channel 中，等待重新传递。 六、Flume 采集数据会丢失吗?根据 Flume 的架构原理，Flume 是不会丢失数据的，其内部有完善的事务机制，Source 到 Channel 是事务性的，Channel 到 Sink 是事务性的，因此这两个环节不会出现数据丢失，唯一的可能性是 Channel 采用的是 memoryChannel，agent 宕机导致数据丢失，或者 Channel 存储数据已满，导致 Source 无法写入，造成数据丢失。 Flume 不会丢失数据，但是有可能造成数据的重复，例如数据已经成功由 Sink 发出，但是没有接收到响应，Sink 会再次发送数据，此时可能会导致数据的重复。","link":"/2020/03/25/Flume%E4%BC%81%E4%B8%9A%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"title":"Flume参数解析","text":"一、启动命令 参数 描述 agent（必填） 运行一个Flume Agent –conf,-c 指定配置文件放在什么目录 –conf-file,-f （必填） 指定配置文件，这个配置文件必须在全局选项的–conf参数定义的目录下 –name,-n （必填） Agent的名字，注意：要和配置文件里的名字一致 -Dproperty=value 设置一个JAVA系统属性值。常见的：-Dflume.root.logger=INFO,console 二、SourceNetCat Source一个NetCat Source用来监听一个指定端口，并接收监听到的数据，接收的数据是字符串形式 配置项 说明 channels （必填） 绑定通道 type (必填) netcat bind （必填） 需要监听的主机名或IP地址 port （必填） 要监听的端口号 selector.* 选择器配置 interceptors.* 拦截器配置 123a1.sources.r1.type=netcata1.sources.r1.bind=0.0.0.0a1.sources.r1.port=44444 Avro Source通过监听Avro端口来接收外部avro客户端发送的日志信息，avro-source接收到的是经过avro序列化后的数据，然后反序列化数据继续传输。源数据必须是经过avro序列化后的数据。利用avro source可以实现多级流动、扇出流、扇入流等效果。 配置项 说明 channels （必填） 绑定通道 type (必填) avro bind （必填） 需要监听的主机名或IP地址 port （必填） 要监听的端口号 threads 工作线程最大线程数 selector.* 选择器配置 interceptors.* 拦截器配置 123a1.sources.r1.type=avroa1.sources.r1.bind=0.0.0.0a1.sources.r1.port=44444 Exec Source可以将命令产生的输出作为源来进行传递 配置项 说明 channels （必填） 绑定通道 type (必填) exec command(必填) 要执行的命令 shell 运行命令的shell脚本 selector.* 选择器配置 interceptors.* 拦截器配置 123a1.sources.r1.type=execa1.sources.r1.command=tail -F /tmp/root/hive.loga1.sources.r1.shell=/bin/bash -c Spooling Directory Sourceflume会持续监听指定的目录，把放入这个目录中的文件当做source来处理 注意： 一旦文件被放到”自动收集”目录后，变不能修改，如果修改，flume会报错。 此外，也不能有重名的文件，如果有，flume也会报错 配置项 说明 channels （必填） 绑定通道 type (必填) spooldir spoolDir （必填） 读被监控的文件夹目录 selector.* 选择器配置 interceptors.* 拦截器配置 12a1.sources.r1.type=spooldira1.sources.r1.spoolDir=/home/work/data HTTP Source此source接收Http的GET和POST请求作为flume的事件，如果想让flume正确解析http协议信息，比如解析出请求头、请求体等信息，需要提供一个可插拔的“处理器”来将请求转化为事件对象，这个处理器必须实现HTTPSourceHeadler接口，这个处理器接收一个HttpServletRequest对象，并返回一个Flume event对象集合。 常用的Handler JSONHandler：可以处理JSON格式的数据，并支持UTF-8(默认),UTF-16,UTF-32字符集，该handler接受event数组，并根据请求头中指定的编码将其转化为Flume Event。 BlobHandler：一种将请求中上传文件信息转化为event的处理器，适合大文件的传输 配置项 说明 channels （必填） 绑定通道 type (必填) http port（必填） 端口 selector.* 选择器配置 interceptors.* 拦截器配置 12345a1.sources.r1.type=httpa1.sources.r1.port=8888a1.sources.r1.bind=192.168.1.100执行curl命令，可以模拟http请求：curl -X POST -d '[{\"headers\":{\"a\":\"a1\",\"b\":\"b1\"},\"body\":\"hello http flume\"}]' http://192.168.1.100:8888 三、ChannelsMemory Channel事件将被存储在内存中（指定大小的队列里），非常适合那些需要高吞吐量且允许数据丢失的场景下 配置项 说明 type (必填) memory capacity 存储在channel中的最大事件数，建议实际工作调节：10万 transactionCapacity 每一个事务中的最大事件数，建议实际工作调节：1000~3000 123a1.channels.c1.type=memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity=100 File Channel将数据临时存储到计算机的磁盘的文件中，性能比较低，但是即使程序出错数据也不会丢失 配置项 说明 type (必填) file dataDirs（必填） 指定存放的目录 12a1.channels.c1.type=filea1.channels.c1.dataDirs=/home/filechannel 四、SinkLogger Sink记录指定级别（如INFO,DEBUG,ERROR）的日志,通常用于调试,根据设计logger sink 将body内容限制为16字节，从而避免屏幕充斥着过多的内容。如果想要查看调试的完整内容，可以使用其他sink，如file_roll sink 它会将日志写到本地文件系统中。 配置项 说明 channels （必填） 绑定通道 type (必填) logger 1a1.sinks.s1.type=logger HDFS Sink此sink将事件写入到hadoop分布式文件系统HDFS中，目前支持text 和 sequence files两种文件格式，支持压缩，并可以对数据进行分区，分桶存储。 配置项 说明 channels （必填） 绑定通道 type (必填) hdfs hdfs.path（必填） HDFS目录路径（如：hdfs://namenode/flume/webdata） hdfs.fileType 文件格式（SequenceFile/DataStream/CompressedStream） hdfs.codeC 文件压缩格式（gzip,bzip2,lzo,lzop,snappy）,文件格式为CompressedStream时必须指定 hdfs.filePrefix 上传文件的前缀 hdfs.fileSuffix 上传文件的后缀 hdfs.inUsePrefix Flume正在处理的文件所加的前缀 hdfs.inUseSuffix Flume正在处理的文件所加的后缀,默认.tmp hdfs.round true 是否按照时间滚动文件夹 hdfs.roundValue 多少时间单位创建一个新的文件夹，默认为1 hdfs.roundUnit 重新定义时间单位 （second,minute,hour）,默认second hdfs.timeZone 时区，默认为Local Time hdfs.useLocalTimeStamp 是否使用本地时间戳，默认为false hdfs.rollInterval 文件生成的时间间隔（秒），默认是30秒,为0 表示不根据时间来滚动文件 hdfs.rollSize 生成的文件大小（字节），默认是1024个字节，为0表示不根据文件大小来滚动文件 hdfs.rollCount 每写几条数据就生成一个新文件，默认数量为10，为0表示不根据event数量来滚动文件 hdfds.batchSize 每个批次刷新到HDFS上的events数量 hdfs.maxOpenFiles 最大允许打开的HDFS文件数，默认5000，超过这个值时，最早打开的文件会被关闭 hdfs.minBlockReplicas HDFS副本数，写入HDFS文件块的最小副本数，该值会影响文件的滚动配置，一般为1，才可以按照配置正确滚动文件 1234a1.sinks.s1.type=hdfsa1.sinks.s1.hdfs.path=hdfs://192.168.1.100:9000/flumea1.sinks.s1.hdfs.fileType = DataStreama1.sinks.s1.hdfs.filePrefix = logs- File_roll Sink在本地系统中存储事件，每隔指定时长生成文件保存这段时间内收集到的日志信息 配置项 说明 channels （必填） 绑定通道 type (必填) file_roll sink.directory （必填） 文件被存储的目录 sink.rollInterval 每隔几秒生成一个新的日志文件。如果为0，则禁止滚动，从而导致所有数据都被写到同一个文件中 123a1.sinks.s1.type=file_rolla1.sinks.s1.sink.directory=/home/work/rolldataa1.sinks.s1.sink.rollInterval=60 Avro Sink将源数据利用avro进行序列化之后写到指定的节点上，是实现多级流动、扇出流、扇入流的基础 配置项 说明 channels （必填） 绑定通道 type (必填) avro hostname（必填） 要发送的主机 port（必填） 要发往的端口号 123a1.sinks.s1.type=avroa1.sinks.s1.hostname=192.168.1.100a1.sinks.s1.port=4141 五、SelectorReplicating Selector复制模式，Selector默认的模式，当source接收到数据后，会复制多分，分发给每一个avro sink 配置项 说明 selector.type replicating Multiplexing Selector多路复用模式，用户可以指定转发的规则。selector根据规则进行数据的分发 配置项 说明 selector.type multiplexing 表示路由模式 selector.header 指定要监测的头的名称 selector.mapping.* 匹配规则 selector.sefault 如果未满足匹配规则，则默认发往指定的通道 1234567a1.sources.r1.selector.type=multiplexinga1.sources.r1.selector.header=statea1.sources.r1.selector.mapping.cn=c1a1.sources.r1.selector.mapping.us=c2a1.sources.r1.selector.mapping.default=c2 测试：curl -X POST -d '[{\"headers\":{\"state\":\"jp\"},\"body\":\"hello flume\"}]' http://0.0.0.0:8888 六、InterceptorTimestamp Interceptor这个拦截器在事件头中插入以毫秒为单位的当前处理时间，头的名字为timestamp，值为当前处理的时间戳，如果在之前已经有这个时间戳，则保留原有的时间戳 配置项 说明 type (必填) timestamp preserveExisting false 如果时间戳已经存在是否保留 1234a1.sources.r1.interceptors=i1a1.sources.r1.interceptors.i1.type=timestampEvent:{headers:{state=jp,timestamp:1472280180424} body: 69 64 6F 61 6C 2E 6F 72 67 5F 62 6F 64 79 idoall.org_body} Host Interceptor这个拦截器插入当前处理Agent的主机名或ip,头的名字为host或者配置的名称，值为主机名或IP地址，基于配置 配置项 说明 type (必填) host preserveExisting false 如果主机名已经存在是否保留 useIP true 如果配置为true则用IP，为false则用主机名 hostHeader host 加入头时使用的名称 1234a1.sources.r1.interceptors=i1a1.sources.r1.interceptors.i1.type=hostEvent:{headers:{ host=127.0.0.1,state=jp} body: 69 64 6F 61 6C 2E 6F 72 67 5F 62 6F 64 79 idoall.org_body} Static Interceptor此拦截器允许用户增加静态头信息使用静态的值到所有事件，目前的实现中不允许一次指定多个头，如果需要增加多个静态头可以指定多个Static interceptors 配置项 说明 type (必填) static key （必填） key 要增加的头名 value（必填） value 要增加的头值 preserveExisting true 123456a1.sources.r1.interceptors=i1a1.sources.r1.interceptors.i1.type=statica1.sources.r1.interceptors.i1.key=addra1.sources.r1.interceptors.i1.value=beijingEvent:{headers:{ host=127.0.0.1,state=jp,addr=beijing} body: 69 64 6F 61 6C 2E 6F 72 67 5F 62 6F 64 79 idoall.org_body} UUID Interceptor这个拦截器在所有事件头中增加一个全局一致性标志，其实就是UUID 配置项 说明 type (必填) org.apache.flume.sink.solr.morphine.UUIDInterceptor$Builder headerName id 头名称 preserveExisting true 如果头已经存在是否保留 prefix “” 在UUID前拼接的字符串前缀 1234a1.sources.r1.interceptors=i1a1.sources.r1.interceptors.i1.type=org.apache.flume.sink.solr.morphine.UUIDInterceptor$BuilderEvent:{headers:{ host=127.0.0.1,state=jp,addr=beijing,id=d354d2f0-14ff-4815-b382-99ae0a191926} body: 69 64 6F 61 6C 2E 6F 72 67 5F 62 6F 64 79 idoall.org_body} Search And Replace Interceptor这个拦截器提供了简单的基于字符串的正则搜索和替换功能 配置项 说明 type (必填) search_replace searchPattern（必填） 要搜索和替换的正则表达式 replaceString（必填） 要替换为的字符串 charset UTF-8 字符编码，默认utf-8 123456a1.sources.r1.interceptors=i1a1.sources.r1.interceptors.i1.type = search_replacea1.sources.r1.interceptors.i1.searchPattern = [0-9]a1.sources.r1.interceptors.i1.replaceString = *Event:{headers:{ host=127.0.0.1,state=jp,addr=beijing,id=d354d2f0-14ff-4815-b382-99ae0a191926} body: 61 62 63 2A 2A 2A 79 2A 2A 2A 64 65 66 abc***y***def} Regex Filtering Interceptor此拦截器通过解析事件体去匹配正则表达式来筛选事件，所提供的正则表达式既可以用来包含或者刨除事件 配置项 说明 type (必填) regex_filter regex（必填） “.*” 所要匹配的正则表达式 excludeEvents 如果为true则刨除匹配事件，为false则包含匹配事件 123456789# 结果将过滤以jp开头的信息，即如果发送的是以jp开头的信息，则收不到a1.sources.r1.interceptors=i1a1.sources.r1.interceptors.i1.type = regex_filtera1.sources.r1.interceptors.i1.regex = ^jp.*$a1.sources.r1.interceptors.i1.excludeEvents = true测试：[root@hadoop100 ~]# curl -X POST -d '[{\"headers\":{\"state\":\"cn\"},\"body\":\"jpabc123y321def\"}]' http://192.168.1.100:44444[root@hadoop100 ~]# Regex Extractor Interceptor使用指定正则表达式匹配事件，并将匹配的组作为头加入到事件中，它也支持插件化的序列化器用来格式化匹配到的组在加入他们作为头之前 配置项 说明 type (必填) regex_extractor regex（必填） 要匹配的正则表达式 serializers（必填） 匹配对象列表 123456789101112a1.sources.r1.interceptors=i1a1.sources.r1.interceptors.i1.type = regex_extractora1.sources.r1.interceptors.i1.regex = (^[a-zA-Z]*)\\\\s([0-9]*$) # regex匹配并进行分组，匹配结果将有两部分，注意\\s空白字符要进行转义a1.sources.r1.interceptors.i1.serializers = s1 s2a1.sources.r1.interceptors.i1.serializers.s1.name = worda1.sources.r1.interceptors.i1.serializers.s2.name = digital测试：curl -X POST -d '[{\"headers\":{},\"body\":\"zhangsan 1234\"}]' http://192.168.1.100:44444结果：Event:{headers:{ word=zhangsan,digital=1234} body:73 68 61 6E 67 20 31 32 33 34 zhangsan 1234}","link":"/2020/03/25/Flume%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90/"},{"title":"Flume案例实操","text":"监控端口数据实时读取本地文件到hdfs实时读取目录文件到HDFS单数据源多出口（选择器）单数据源多出口（sink组）多数据源汇总 监控端口数据案例需求首先，Flume监控本机44444端口，然后通过nc工具向本机44444端口发送消息，最后Flume将监听的数据实时显示在控制台。 需求分析 通过nc工具向本机的44444端口发送数据 Flume监控本机的44444端口，通过Flume的source端读取数据 Flume将获取的数据通过sink端写出到控制台实现步骤 将rpm软件包烤入/opt/software文件夹下面，执行rpm软件包安装命令，安装nc工具1[root@hadoop100 software]# rpm -ivh nc-1.84-24.el6.x86_64.rpm 判断4444端口是否被占用1[root@hadoop100 software]# netstat -tunlp | grep 44444 netstat 语法：参数 | 说明 —|—–tcp,-t | 显示TCP传输协议的连接状况–udp,-u | 显示UDP传输协议的连接状况–numeric,-n | 直接使用IP地址，而不通过域名服务器–listening,-l | 显示监控中的服务器的Socket–programs,-p | 显示正在使用Socket的程序识别码和程序名称 创建Flume Agent配置文件flume-nc-logger.conf 在flume目录下创建job文件夹并进入job文件夹 12[root@hadoop100 flume-1.6.0]# mkdir job[root@hadoop100 flume-1.6.0]# cd job 在job文件夹下创建Flume Agent配置文件flume-nc-logger.conf,并添加如下内容 1234567891011121314151617181920212223[root@hadoop100 job]# touch flume-nc-logger.conf[root@hadoop100 job]# vim flume-nc-logger.conf # Name the components on this agent # a1表示agent的名称a1.sources = r1 # r1表示a1的输入源a1.sinks = k1 # k1表示a1的输出目的地a1.channels = c1 # c1表示a1的缓冲区# Describe/configure the sourcea1.sources.r1.type = netcat # 表示a1的输入源类型为netcat端口类型a1.sources.r1.bind = localhost # 表示a1监听的主机a1.sources.r1.port = 44444 # 表示a1监听的端口号# Describe the sink a1.sinks.k1.type = logger # 表示a1的输出目的地是控制台logger类型# Use a channel which buffers events in memorya1.channels.c1.type = memory # 表示a1的channel类型是memory内存型a1.channels.c1.capacity = 1000 # 表示a1的channel总容量为1000个eventa1.channels.c1.transactionCapacity = 100 # 表示a1的channel传输时收集到了100个event后再去提交事务# Bind the source and sink to the channela1.sources.r1.channels = c1 # 表示将r1和c1连接起来a1.sinks.k1.channel = c1 # 表示将k1和c1连接起来 开启flume监听1[root@hadoop100 job]# flume-ng agent -n a1 -c ./ -f ./flume-nc-logger.conf -Dflume.root.logger=INFO,console 启动命令：参数 | 描述 —|—agent | 运行一个Flume Agent–conf,-c | 指定配置文件放在什么目录–conf-file,-f (必填)| 指定配置文件，这个配置文件必须在全局选项的–conf参数定义的目录下–name,-n (必填)| Agent的名字，注意：要和配置文件里的名字一致-Dproperty=value | 设置一个JAVA系统属性值。常见的：-Dflume.root.logger=INFO,console,-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别，日志级别包括：log、info、warn、error 通过nc工具向本机的44444端口发送内容 123[root@hadoop100 ~]# nc localhost 44444hello flumeOK 或者通过外部http请求访问对应的IP和端口，如：http://192.168.1.100:44444/hello 在Flume监听页面观察接收数据情况 12020-04-12 13:11:48,650 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:94)] Event: { headers:{} body: 68 65 6C 6C 6F 20 66 6C 75 6D 65 hello flume } 实时读取本地文件到hdfs 案例需求实时监控Hive日志，并上传到HDFS中 需求分析 创建符合条件的flume配置文件 执行配置文件，开启监控 开启Hive,生成日志 查看HDFS上数据 实现步骤 Flume要想将数据输出到HDFS，必须持有Hadoop相关jar包12345commons-configuration-1.6.jarhadoop-auth-2.7.1.jarhadoop-comment-2.7.1.jarhadoop-hdfs-2.7.1.jarhadoop-marreduce-client-core-2.7.1.jar 创建flume-file-hdfs.conf文件,并添加如下内容： 123456789101112131415161718192021222324[root@hadoop100 job]# touch flume-file-hdfs.conf[root@hadoop100 job]# vim flume-file-hdfs.conf# Name the components on this agenta2.sources = r2a2.sinks = k2a2.channels = c2# Describe/configure the sourcea2.sources.r2.type = execa2.sources.r2.command = tail -F /tmp/root/hive.log# Describe the sinka2.sinks.k2.type = hdfsa2.sinks.k2.hdfs.path=hdfs://hadoop100:9000/flume/%Y%m%d/%Ha2.sinks.k2.hdfs.useLocalTimeStamp = true# Use a channel which buffers events in memorya2.channels.c2.type = memorya2.channels.c2.capacity = 1000a2.channels.c2.transactionCapacity = 100# Bind the source and sink to the channela2.sources.r2.channels = c2a2.sinks.k2.channel = c2 执行监控配置1[root@hadoop100 job]# flume-ng agent -n a2 -f ./flume-file-hdfs.conf 开启Hadoop和Hive并操作hive产生日志12[root@hadoop100 ~]# start-all.sh[root@hadoop100 ~]# hive 在HDFS上查看文件1[root@hadoop100 bin]# hadoop dfs -cat /flume/20200412/14/FlumeData.1586674705784 实时读取目录文件到HDFS 案例需求使用Flume监控整个目录的文件 需求分析 创建符合条件的flume配置文件 执行配置文件，开启监控 向upload目录中添加文件 查看HDFS上数据 查看/opt/module/flume/upload目录中上传的文件是否已经标记为.COMPLETED结尾；.tmp后缀结尾文件没有上传 实现步骤 创建配置文件flume-dir-hdfs.conf,并添加如下内容123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657a3.sources=r3a3.sinks=k3a3.channels=c3#Describe/configure the sourcea3.sources.r3.type=spooldira3.sources.r3.spoolDir=/opt/module/flume/uploada3.sources.r3.fileSuffix=.COMPLETEDa3.sources.r3.fileHeader=true#忽略所有以.tmp结尾的文件，不上传a3.sources.r3.ignorePattern=([^]*\\.tmp)# Describe the sinka3.sinks.k3.type=hdfsa3.sinks.k3.hdfs.path=hdfs://hadoop100:9000/flume/upload/%Y%m%d/%H#是否使用本地时间戳a3.sinks.k3.hdfs.useLocalTimeStamp=true#上传文件的前缀a3.sinks.k3.hdfs.filePrefix=upload-#是否按照时间滚动文件夹a3.sinks.k3.hdfs.round=true#多少时间单位创建一个新的文件夹a3.sinks.k3.hdfs.roundValue=1#重新定义时间单位a3.sinks.k3.hdfs.roundUnit=hour#积攒多少个Event才flush到HDFS一次a3.sinks.k3.hdfs.batchSize=100#设置文件类型，可支持压缩a3.sinks.k3.hdfs.fileType=DataStream#多久生成一个新的文件a3.sinks.k3.hdfs.rollInterval=600#设置每个文件的滚动大小大概是128Ma3.sinks.k3.hdfs.rollSize=134217700#文件的滚动与Event数量无关a3.sinks.k3.hdfs.rollCount=0# 最小冗余数a3.sinks.k3.hdfs.minBlockReplicas=1# Useachannelwhichbufferseventsinmemorya3.channels.c3.type=memorya3.channels.c3.capacity=1000a3.channels.c3.transactionCapacity=100# Bindthesourceandsinktothechannela3.sources.r3.channels=c3a3.sinks.k3.channel=c3 启动监控文件夹命令1[root@hadoop100 job]# flume-ng agent -n a3 -f ./flume-dir-hdfs.conf 说明：在使用Spooling Directory Source时 （1）不要在监控目录中创建并持续修改文件（2）上传完成的文件会以.COMPLETED结尾（3）被监控文件夹每500毫秒扫描一次文件变动3. 向upload文件夹中添加文件 1234[root@hadoop100 flume-1.6.0]# mkdir load[root@hadoop100 upload]# touch bgnv5.txt[root@hadoop100 upload]# touch bgnv5.tmp[root@hadoop100 upload]# touch bgnv5.log 查看HDFS上的数据1[root@hadoop100 ~]# hadoop dfs -ls /flume/upload/20200412/19 查看upload文件夹12[root@hadoop100 upload]# lsbgnv5.log.COMPLETED bgnv5.tmp bgnv5.txt.COMPLETED 单数据源多出口（选择器） 案例需求使用flume-1监控文件变动，flume-1将变动内容传递给flume-2，flume-2负责存储到HDFS。同时flume-1将变动内容传递给flume-3，flume-3负责输出到local filesystem。 实现步骤 准备工作 在/opt/module/flume-1.6.0/job目录下创建group1 文件夹 1[root@hadoop100 group1]# cd /opt/module 在/opt/module/datas/目录下创建flume3文件夹 1[root@hadoop100 datas]# mkdir flume3 创建flume-file-flume.conf，并添加如下内容 配置1个接收日志文件的source和两个channel、两个sink，分别输送给flume-flume-hdfs和flume-flume-dir。 12345678910111213141516171819202122232425262728293031323334353637[root@hadoop100 group1]$ touch flume-file-flume.conf[root@hadoop100 group1]$ vim flume-file-flume.conf# Name the components on this agenta1.sources = r1a1.sinks = k1 k2a1.channels = c1 c2 # 将数据流复制给多个channela1.sources.r1.selector.type = replicating # Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /tmp/root/hive.loga1.sources.r1.shell = /bin/bash -c # Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = hadoop100a1.sinks.k1.port = 4141a1.sinks.k2.type = avroa1.sinks.k2.hostname = hadoop100a1.sinks.k2.port = 4142 # Describe the channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.channels.c2.type = memorya1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channela1.sources.r1.channels = c1 c2a1.sinks.k1.channel = c1a1.sinks.k2.channel = c2 创建flume-flume-hdfs.conf，并添加如下内容配置上级Flume输出的Source,输出是到HDFS的sink1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@hadoop100 group1]$ touch flume-flume-hdfs.conf[root@hadoop100 group1]$ vim flume-flume-hdfs.conf# Name the components on this agenta2.sources = r1a2.sinks = k1a2.channels = c1 # Describe/configure the sourcea2.sources.r1.type = avroa2.sources.r1.bind = hadoop100a2.sources.r1.port = 4141 # Describe the sinka2.sinks.k1.type = hdfsa2.sinks.k1.hdfs.path = hdfs://hadoop100:9000/flume2/%Y%m%d/%H#是否使用本地时间戳a2.sinks.k1.hdfs.useLocalTimeStamp = true#上传文件的前缀a2.sinks.k1.hdfs.filePrefix = flume2- #是否按照时间滚动文件夹a2.sinks.k1.hdfs.round = true #多少时间单位创建一个新的文件夹a2.sinks.k1.hdfs.roundValue = 1 #重新定义时间单位a2.sinks.k1.hdfs.roundUnit = hour#积攒多少个Event才flush到HDFS一次a2.sinks.k1.hdfs.batchSize = 100 #设置文件类型，可支持压缩a2.sinks.k1.hdfs.fileType = DataStream #多久生成一个新的文件a2.sinks.k1.hdfs.rollInterval = 600 #设置每个文件的滚动大小大概是128Ma2.sinks.k1.hdfs.rollSize = 134217700 #文件的滚动与Event数量无关a2.sinks.k1.hdfs.rollCount = 0 #最小冗余数a2.sinks.k1.hdfs.minBlockReplicas = 1 # Describe the channela2.channels.c1.type = memorya2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela2.sources.r1.channels = c1a2.sinks.k1.channel = c1 创建flume-flume-dir.conf,并添加如下内容 配置上级flume输出的source，输出是到本地目录的sink。 123456789101112131415161718192021222324[root@hadoop100 group1]$ touch flume-flume-dir.conf[root@hadoop100 group1]$ vim flume-flume-dir.conf# Name the components on this agenta3.sources = r1a3.sinks = k1a3.channels = c2 # Describe/configure the sourcea3.sources.r1.type = avroa3.sources.r1.bind = hadoop100a3.sources.r1.port = 4142 # Describe the sinka3.sinks.k1.type = file_rolla3.sinks.k1.sink.directory = /opt/module/datas/flume3 # Describe the channela3.channels.c2.type = memorya3.channels.c2.capacity = 1000a3.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channela3.sources.r1.channels = c2a3.sinks.k1.channel = c2 提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。4. 执行配置文件 分别开启对应配置文件：flume-flume-dir，flume-flume-hdfs，flume-file-flume。 123[root@hadoop100 flume]$ bin/flume-ng agent -n a3 -f job/group1/flume-flume-dir.conf[root@hadoop100 flume]$ bin/flume-ng agent -n a2 -f job/group1/flume-flume-hdfs.conf[root@hadoop100 flume]$ bin/flume-ng agent -n a1 -f job/group1/flume-file-flume.conf 启动Hadoop和Hive12[root@hadoop100 ~]# start-all.sh[root@hadoop100 ~]# hive 检查HDFS上数据123456[root@hadoop100 ~]# hadoop dfs -ls /flume2/20200412/19DEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.Found 1 items-rw-r--r-- 1 root supergroup 1563 2020-04-12 19:30 /flume2/20200412/19/flume2-.1586690993114.tmp 检查/opt/module/datas/flume3目录中的数据123[root@hadoop100 flume3]# ll总用量 8-rw-r--r-- 1 root root 0 4月 12 19:26 1586690789923-1 单数据源多出口（sink组） 案例需求使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2,Flume-2负责存储到HDFS。同时Flume-1将变动内容传递给Flume-3,Flume-3也负责存储到HDFS 实现步骤 准备工作 在/opt/module/flume/job目录下创建一个group2文件夹 1[root@hadoop100 job]$ mkdir group2 创建flume-file2-flume.conf，并添加如下内容： 配置1个接收日志文件的source和1个channel、两个sink,分别输送给flume-flume-hdfs1和flume-flume-hdfs2 12345678910111213141516171819202122232425262728293031323334353637[root@hadoop100 group2]$ touch flume-file2-flume.conf[root@hadoop100 group2]$ vim flume-file2-flume.conf# Name the components on this agenta1.sources = r1a1.channels = c1a1.sinkgroups = g1a1.sinks = k1 k2# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /tmp/root/hive.loga1.sources.r1.shell = /bin/bash -ca1.sinkgroups.g1.processor.type = load_balancea1.sinkgroups.g1.processor.backoff = truea1.sinkgroups.g1.processor.selector = round_robina1.sinkgroups.g1.processor.selector.maxTimeOut=10000# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = hadoop100a1.sinks.k1.port = 4141a1.sinks.k2.type = avroa1.sinks.k2.hostname = hadoop100a1.sinks.k2.port = 4142# Describe the channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinkgroups.g1.sinks = k1 k2a1.sinks.k1.channel = c1a1.sinks.k2.channel = c1 创建flume-flume-hdfs1.conf，并添加如下内容 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@hadoop100 group1]$ touch flume-flume-hdfs1.conf[root@hadoop100 group1]$ vim flume-flume-hdfs1.conf# Name the components on this agenta2.sources = r1a2.sinks = k1a2.channels = c1# Describe/configure the sourcea2.sources.r1.type = avroa2.sources.r1.bind = hadoop100a2.sources.r1.port = 4141# Describe the sinka2.sinks.k1.type = hdfsa2.sinks.k1.hdfs.path = hdfs://hadoop100:9000/flume3/%Y%m%d/%H#是否使用本地时间戳a2.sinks.k1.hdfs.useLocalTimeStamp = true#上传文件的前缀a2.sinks.k1.hdfs.filePrefix = flume3- #是否按照时间滚动文件夹a2.sinks.k1.hdfs.round = true #多少时间单位创建一个新的文件夹a2.sinks.k1.hdfs.roundValue = 1 #重新定义时间单位a2.sinks.k1.hdfs.roundUnit = hour#积攒多少个Event才flush到HDFS一次a2.sinks.k1.hdfs.batchSize = 100 #设置文件类型，可支持压缩a2.sinks.k1.hdfs.fileType = DataStream #多久生成一个新的文件a2.sinks.k1.hdfs.rollInterval = 600 #设置每个文件的滚动大小大概是128Ma2.sinks.k1.hdfs.rollSize = 134217700 #文件的滚动与Event数量无关a2.sinks.k1.hdfs.rollCount = 0 #最小冗余数a2.sinks.k1.hdfs.minBlockReplicas = 1# Describe the channela2.channels.c1.type = memorya2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela2.sources.r1.channels = c1a2.sinks.k1.channel = c1 创建flume-flume-hdfs2.conf，并添加如下内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[root@hadoop100 group1]$ touch flume-flume-hdfs2.conf[root@hadoop100 group1]$ vim flume-flume-hdfs2.conf# Name the components on this agenta3.sources = r1a3.sinks = k1a3.channels = c2# Describe/configure the sourcea3.sources.r1.type = avroa3.sources.r1.bind = hadoop100a3.sources.r1.port = 4142# Describe the sinka2.sinks.k1.type = hdfsa2.sinks.k1.hdfs.path = hdfs://hadoop100:9000/flume3/%Y%m%d/%H#是否使用本地时间戳a2.sinks.k1.hdfs.useLocalTimeStamp = true#上传文件的前缀a2.sinks.k1.hdfs.filePrefix = flume3- #是否按照时间滚动文件夹a2.sinks.k1.hdfs.round = true #多少时间单位创建一个新的文件夹a2.sinks.k1.hdfs.roundValue = 1 #重新定义时间单位a2.sinks.k1.hdfs.roundUnit = hour#积攒多少个Event才flush到HDFS一次a2.sinks.k1.hdfs.batchSize = 100 #设置文件类型，可支持压缩a2.sinks.k1.hdfs.fileType = DataStream #多久生成一个新的文件a2.sinks.k1.hdfs.rollInterval = 600 #设置每个文件的滚动大小大概是128Ma2.sinks.k1.hdfs.rollSize = 134217700 #文件的滚动与Event数量无关a2.sinks.k1.hdfs.rollCount = 0 #最小冗余数a2.sinks.k1.hdfs.minBlockReplicas = 1# Describe the channela3.channels.c2.type = memorya3.channels.c2.capacity = 1000a3.channels.c2.transactionCapacity = 100# Bind the source and sink to the channela3.sources.r1.channels = c2a3.sinks.k1.channel = c2 执行配置文件分别开启对应配置文件：flume-flume-hdfs2.conf，flume-flume-hdfs1.conf，flume-file2-flume.conf。 123[root@hadoop100 flume]$ bin/flume-ng agent -n a3 -f job/group2/flume-flume-hdfs2.conf[root@hadoop100 flume]$ bin/flume-ng agent -n a2 -f job/group2/flume-flume-hdfs1.conf[root@hadoop100 flume]$ bin/flume-ng agent -n a1 -f job/group2/flume-file2-flume.conf 启动Hadoop和Hive 12[root@hadoop100 ~]# start-all.sh[root@hadoop100 ~]# hive 检查HDFS上数据 123456[root@hadoop100 ~]# hadoop dfs -ls /flume3/20200412/20DEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it.Found 1 items-rw-r--r-- 1 root supergroup 1563 2020-04-12 20:47 /flume3/20200412/20/flume3-.1586695663649.tmp 多数据源汇总 案例需求flume-1监控文件hive.log，flume-2监控某一个端口的数据流，flume-1与flume-2将数据发送给flume-3，flume3将最终数据写入到HDFS 实现步骤 准备工作 分发Flume 1[root@hadoop100 module]$ xsync flume-1.6.0 在hadoop101,hadoop102以及hadoop103的/opt/module/flume/job目录下创建一个group3文件夹 123[root@hadoop101 job]$ mkdir group3[root@hadoop102 job]$ mkdir group3[root@hadoop103 job]$ mkdir group3 在hadoop102上创建flume-file-flume.conf并添加如下内容 配置source用于监控hive.log文件，配置sink输出数据到下一级flume。 12345678910111213141516171819202122232425[root@hadoop102 group3]$ touch flume-file-flume.conf[root@hadoop102 group3]$ vim flume-file-flume.conf# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1 # Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /tmp/root/hive.loga1.sources.r1.shell = /bin/bash -c # Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = hadoop103a1.sinks.k1.port = 4141 # Describe the channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 在hadoop101上创建flume-netcat-flume.conf并添加如下内容 配置source监控端口44444数据流，配置sink数据到下一级flume 12345678910111213141516171819202122232425[root@hadoop101 group3]$ touch flume-netcat-flume.conf[root@hadoop101 group3]$ vim flume-netcat-flume.conf# Name the components on this agenta2.sources = r1a2.sinks = k1a2.channels = c1 # Describe/configure the sourcea2.sources.r1.type = netcata2.sources.r1.bind = hadoop101a2.sources.r1.port = 44444 # Describe the sinka2.sinks.k1.type = avroa2.sinks.k1.hostname = hadoop103a2.sinks.k1.port = 4141 # Use a channel which buffers events in memorya2.channels.c1.type = memorya2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela2.sources.r1.channels = c1a2.sinks.k1.channel = c1 在hadoop103上创建flume-flume-hdfs.conf,并添加如下内容 配置source用于接收flume-file-flume与flume-netcat-flume发送过来的数据流，最终合并后sink到HDFS。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@hadoop103 group3]$ touch flume-flume-hdfs.conf[root@hadoop103 group3]$ vim flume-flume-hdfs.conf# Name the components on this agenta3.sources = r1a3.sinks = k1a3.channels = c1 # Describe/configure the sourcea3.sources.r1.type = avroa3.sources.r1.bind = hadoop103a3.sources.r1.port = 4141 # Describe the sinka3.sinks.k1.type = hdfsa3.sinks.k1.hdfs.path = hdfs://hadoop103:9000/flume4/%Y%m%d/%H#是否使用本地时间戳a3.sinks.k1.hdfs.useLocalTimeStamp = true #上传文件的前缀a3.sinks.k1.hdfs.filePrefix = flume4- #是否按照时间滚动文件夹a3.sinks.k1.hdfs.round = true #多少时间单位创建一个新的文件夹a3.sinks.k1.hdfs.roundValue = 1 #重新定义时间单位a3.sinks.k1.hdfs.roundUnit = hour #积攒多少个Event才flush到HDFS一次a3.sinks.k1.hdfs.batchSize = 100 #设置文件类型，可支持压缩a3.sinks.k1.hdfs.fileType = DataStream #多久生成一个新的文件a3.sinks.k1.hdfs.rollInterval = 600 #设置每个文件的滚动大小大概是128Ma3.sinks.k1.hdfs.rollSize = 134217700 #文件的滚动与Event数量无关a3.sinks.k1.hdfs.rollCount = 0 #最小冗余数a3.sinks.k1.hdfs.minBlockReplicas = 1 # Describe the channela3.channels.c1.type = memorya3.channels.c1.capacity = 1000a3.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela3.sources.r1.channels = c1a3.sinks.k1.channel = c1 执行配置文件 分别开启对应配置文件：flume-flume-hdfs.conf，flume-netcat-flume.conf，flume-file-flume.conf。 123[root@hadoop103 flume]$ bin/flume-ng agent -n a3 -f job/group3/flume-flume-hdfs.conf[root@hadoop101 flume]$ bin/flume-ng agent -n a2 -f job/group3/flume-netcat-flume.conf[root@hadoop102 flume]$ bin/flume-ng agent -n a1 -f job/group3/flume-file-flume.conf 启动hadoop和hive12[root@hadoop102 hadoop-2.7.2]$ start-all.sh[root@hadoop102 hive]$ bin/hive 向44444端口发送数据1[root@hadoop102 flume]$ nc hadoop102 44444 检查HDFS上数据1[root@hadoop100 ~]# hadoop dfs -ls /flume4/20200412/20","link":"/2020/03/25/Flume%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D/"},{"title":"Flume基本概念","text":"一、概述 Flume最早是Cloudera提供的日志收集系统，后贡献给Apache Flume是一个高可用的、高可靠的、分布式的海量日志采集、聚合和传输的系统 Flume支持在日志系统中定制各类数据发送方，用于收集数据（source） Flume提供对数据的简单处理，并写到各种数据接收方（可定制）的能力（sink） 二、版本 Flume0.9X：又称Flume-og，老版本的flume，需要引入zookeeper集群管理，性能也比较低（单线程工作） Flume1.X：又称Flume-ng，新版本需要引入zookeeper，和Flume-og不兼容 三、Flume特性 可靠性：事务型的数据传递，保证数据的可靠性。一个日志交给flume来处理，不会出现此日志丢失或未被处理的及情况。 可恢复性：通道可以以内存或文件的方式实现，内存更快，但不可恢复。文件较慢但提供了可恢复性。 四、Flume组成架构 source不断的接收数据，将数据封装成一个一个的event，然后将event发送给channel，channel作为一个缓冲区会临时存放这些event数据，随后sink会将channel中的event数据发送到指定的地方，如hdfs等。当sink将channel中的数据成功发送出去之后，channel会将event数据进行删除，保证了数据传输的可靠性与安全性。 AgentFlume以agent为最小的独立运行单位，一个agent就是一个JVM进程，主要负责将数据以事件的形式从源头送至目的端。它包含三个核心组件，分别是Source、Channel、Sink。 Source主要负责收集数据，将接收到的数据封装到事件（event）中，然后将事件推送channel中。 在这个过程中它会先调用doPut()方法把批数据写入到临时缓冲区putList中，然后会检查channel中的容量是否足够，如果足够的话则会调用doCommit()方法把putList中的数据推送的channel中，如果channel容量不够的话，则会调用doRollBack()方法将数据回滚到putList中。 source可以处理的数据类型包括：netcat、exec、spooling directory、http、avro、thrift、jms、sequence generator、syslog、legacy等 Channel是连接source和sink的组件，它可以将事件暂存到内存中也可以持久化到本地磁盘上，直到sink处理完该事件。它相当于一个数据的缓冲区。 flume自带两种channel：MemoryChannel、FileChannel MemoryChannel：将事件写入到内存中，可以实现高速的吞吐，但是无法保证数据的完整性，因为程序死亡、机器宕机或者重启都会导致数据丢失。 FileChannel：将事件写到磁盘中，虽然效率相对较低，但是保证在程序关闭或者机器宕机的情况下不会丢失数据。 Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。 Sink主要负责从channel中读取事件（event），将事件拉取到相应的存储系统，或者发送到另外一个Flume Agent中，并删除Channel中的缓存数据。 在这个过程中它会先调用 doTake() 方法把数据读取到临时缓冲区takeList中，然后检查数据是否发送成功，如果发送的话则调用 doCommit() 方法，把 event 从 takeList 中移除，如果发送失败，则调用 doRollBack() 方法把takeList中的数据回滚到 Channel 中。 Sink组件的目的地包括：hdfs、logger、avro、thrift、ipc、file、hbase、solr等 eventFlume数据传输的基本单元，以Event的形式将数据从源头送至目的地。它由Header和Body两部分组成，header是拦截器过滤好event之后，给event加的具体的header，用来存放该event的一些属性，为K-V结构，Body用来存放该条数据，形式为字节数组，所以一般都是拦截器和Multiplexing Channel Selector 结合起来使用。 五. Flume拓扑结构单一流程 多个agent的数据流（多级流动） 数据流合并（扇入流）在做日志收集的时候一个常见的场景就是，大量的生产日志的客户端发送数据到少量的附属于存储子系统的消费者agent。例如，从数百个web服务器中收集日志，它们发送数据到十几个负责将数据写入HDFS集群的agent。 这个在Flume中可以实现，需要配置大量第一层的agent，每一个agent都有一个avro sink,让它们都指向同一个agent的avro source（在这样的场景下也可以使用thrift source/sink/client）。在第二层agent上的source将收到的event合并到一个channel中，event被一个sink消费到它的最终目的地。 数据流复用（扇出流）Flume支持多路输出event流到一个或多个目的地。这是靠定义一个多路数据流实现的，它可以实现复制和选择性路由一个event到一个或者多个channel中。 上面的例子展示了agent foo 中source 扇出数据流到三个不同的channel，这个扇出可以是复制或者多路输出。在复制数据流的情况下，每一个event被发送到所有的channel中；在多路输出的情况下，一个event被发送到一部分可用的channel中，它们是根据event的属性和预先配置的值选择channel的。这些映射关系应该被填写在agent的配置文件中。 负载均衡功能 六、Interceptor拦截器拦截器需要实现org.apache.flume.interceptor.Interceptor接口，通过拦截器可以实现在运行阶段修改/删除event。 拦截器采用了责任链模式，多个拦截器可以按指定顺序拦截，一个拦截器返回的事件列表被传递给链中的下一个拦截器。 如果一个拦截器需要删除事件，只需要在返回的事件集中不包含要删除的事件即可。如果要删除所有事件，只需要返回一个空列表。 七、Channel SelectorsChannel选择器有两种类型：Replicating Channel Selector（默认的）和 Multiplexing Channel Selector。 Replicating Channel Selector : 将source过来的events发往所有的channel（相当于复制多份） Multiplexing Channel Selector：可以根据event中header的值来配置具体发往哪一个Channel中。 八、Flume Agent内部原理 Source采集数据，将数据封装成事件对象(event)，然后交给Channel Processor Channel Processor将事件传递给拦截器链进行简单的数据清洗 拦截器链处理完后将数据返回给Channel Processor。 Channel Processor将拦截过滤之后的event事件传递给Channel选择器(Channel Selector)，由channel选择器决定每个event具体分配给哪一个Channel。 Channel Selector返回给Channel Processor写入event事件的Channel列表。 Channel Processor根据Channel选择器的选择结果，将Event事件写入相应的Channel中。 最后SinkProcessor启动sink，sink不断到channel中去轮询，将channel中的event事件拿过来。 九、Flume安装 将apache-flume-1.6.0-bin.tar.gz上传到Linux的/opt/software 目录下 解压apache-flume-1.6.0-bin.tar.gz到/opt/module 目录下1[root@hadoop100 software]# tar -zxvf apache-flume-1.6.0-bin.tar.gz -C /opt/module/ 修改apache-flume-1.6.0-bin 的名称为flume-1.6.01[root@hadoop100 module]# mv apache-flume-1.6.0-bin/ flume-1.6.0 将flume/conf 下的flume-env.sh.template 文件修改为flume-env.sh，并配置flume-env.sh123[root@hadoop100 conf]# mv flume-env.sh.template flume-env.sh[root@hadoop100 conf]# vim flume-env.shexport JAVA_HOME=/opt/module/jdk1.8.0_40 配置环境变量1234[root@hadoop100 conf]# vim ~/.bashrc #FLUMEexport FLUME_HOME=/opt/module/flume-1.6.0export PATH=$PATH:$FLUME_HOME/bin &emsp;&emsp;保存使其立即生效1[root@hadoop100 conf]# source ~/.bashrc 查看Flume版本123456[root@hadoop100 conf]# flume-ng versionFlume 1.6.0Source code repository: https://git-wip-us.apache.org/repos/asf/flume.gitRevision: 2561a23240a71ba20bf288c7c2cda88f443c2080Compiled by hshreedharan on Mon May 11 11:15:44 PDT 2015From source with checksum b29e416802ce9ece3269d34233baf43f","link":"/2020/03/25/Flume%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"},{"title":"Flume监控之Ganglia","text":"一、安装与部署 安装httpd服务与php1[root@hadoop100 flume-1.6.0]# yum -y install httpd php 安装其他依赖12[root@hadoop100 flume-1.6.0]# yum -y install rrdtool perl-rrdtool rrdtool-devel[root@hadoop100 flume-1.6.0]# yum -y install apr-devel 安装ganglia1234567891011//先要制作一个最简单的epel第三方yum安装配置:[root@hadoop100 flume-1.6.0]# vim /etc/yum.repos.d/epel.repo [epel]name=CentOS-$releasever - Epelbaseurl=http://dl.fedoraproject.org/pub/epel/$releasever/$basearch/gpgcheck=0[root@hadoop100 flume-1.6.0]# yum install libconfuse libconfuse-devel -y[root@hadoop100 flume-1.6.0]# yum -y install ganglia-gmetad [root@hadoop100 flume-1.6.0]# yum -y install ganglia-web [root@hadoop100 flume-1.6.0]# yum -y install ganglia-gmond 修改配置文件/etc/httpd/conf.d/ganglia.conf12345678[root@hadoop100 flume-1.6.0]# vim /etc/httpd/conf.d/ganglia.conf //注释掉其他的语句，添加Require all granted&lt;Location /ganglia&gt; Require all granted # Require local # Require ip 10.1.2.3 # Require host example.org&lt;/Location&gt; 修改配置文件/etc/ganglia/gmetad.conf123//修改data_source[root@hadoop100 flume-1.6.0]# vim /etc/ganglia/gmetad.conf data_source \"hadoop100\" 192.168.1.100 修改配置文件/etc/ganglia/gmond.conf12345678910111213141516171819202122232425262728293031323334[root@hadoop100 flume-1.6.0]# vim /etc/ganglia/gmond.conf //修改namecluster { name = \"hadoop100\" owner = \"unspecified\" latlong = \"unspecified\" url = \"unspecified\"}//注释掉mcast_join,修改hostudp_send_channel { #bind_hostname = yes # Highly recommended, soon to be default. # This option tells gmond to use a source address # that resolves to the machine's hostname. Without # this, the metrics may appear to come from any # interface and the DNS names associated with # those IPs will be used to create the RRDs. # mcast_join = 239.2.11.71 host = 192.168.1.100 port = 8649 ttl = 1}//注释掉mcast_join，添加bindudp_recv_channel { # mcast_join = 239.2.11.71 port = 8649 bind = 192.168.1.100 retry_bind = true # Size of the UDP buffer. If you are handling lots of metrics you really # should bump it up to e.g. 10MB or even higher. # buffer = 10485760} 修改配置文件/etc/selinux/config123[root@hadoop100 flume-1.6.0]# vim /etc/selinux/config//修改SELINUXSELINUX=disabled 启动ganglia123[root@hadoop100 flume-1.6.0]# systemctl start httpd.service[root@hadoop100 flume-1.6.0]# systemctl start gmetad.service[root@hadoop100 flume-1.6.0]# systemctl start gmond.service 打开网页浏览ganglia页面：http://192.168.1.100/ganglia 注意：如果完成以上操作依然出现权限不足错误，请修改/var/lib/ganglia目录的权限 1[root@hadoop100 flume-1.6.0]# chmod -R 777 /var/lib/ganglia 二、操作Flume测试监控 修改/opt/module/flume/conf目录下的flume-env.sh配置 123456[root@hadoop100 flume-1.6.0]# vim /opt/module/flume-1.6.0/conf/flume-env.sh //添加如下配置JAVA_OPTS=\"-Dflume.monitoring.type=ganglia-Dflume.monitoring.hosts=192.168.1.100:8649-Xms100m-Xms200m\" 启动Flume任务 123456[root@hadoop100 job]# flume-ng agent \\-n a1 \\-f flume-nc-logger.conf \\-Dflume.root.logger==INFO,console \\-Dflume.monitoring.type=ganglia \\-Dflume.monitoring.hosts=192.168.1.100:8649 发送数据观察ganglia监测图 1[root@hadoop100 ~]# nc localhost 44444 图例说明： 字段（图标名称） 字段含义 EventPutAttemptCount source尝试写入channel的时间总数量 EventPutSuccessCount 成功写入channel且提交的事件总数量 EventTakeAttemptCount sink尝试从channel拉取事件的总数量，这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据 EventTakeSuccessCount sink成功读取的事件的总数量 StartTime channel 启动的时间（毫秒） StopTime channel 停止的时间（毫秒） ChannelSize 目前channel中事件的总数量 ChannelFillPercentage channel 占用百分比 ChannelCapacity channel 的总容量","link":"/2020/03/25/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/"}],"tags":[{"name":"test","slug":"test","link":"/tags/test/"},{"name":"LeetCode","slug":"LeetCode","link":"/tags/LeetCode/"},{"name":"hive","slug":"hive","link":"/tags/hive/"},{"name":"Flume","slug":"Flume","link":"/tags/Flume/"}],"categories":[{"name":"test","slug":"test","link":"/categories/test/"},{"name":"LeetCode","slug":"LeetCode","link":"/categories/LeetCode/"},{"name":"hive","slug":"hive","link":"/categories/hive/"},{"name":"Flume","slug":"Flume","link":"/categories/Flume/"}]}